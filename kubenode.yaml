heat_template_version: 2013-05-23

description: >
  This is a nested stack that defines a single Kubernetes minion,
  based on a vanilla Fedora 20 cloud image.  This stack is included by
  a ResourceGroup resource in the parent template (kubecluster.yaml).

parameters:

  server_image:
    type: string
    default: fedora-20-x86_64-updated
    description: glance image used to boot the server

  server_flavor:
    type: string
    default: m1.small
    description: flavor to use when booting the server

  ssh_key_name:
    type: string
    description: name of ssh key to be provisioned on our server
    default: lars

  external_network_id:
    type: string
    description: uuid of a network to use for floating ip addresses

  allow_priv:
    type: string
    description: >
      whether or not kubernetes should permit privileged containers.
    default: "false"
    constraints:
      - allowed_values: ["true", "false"]

  # The following are all generated in the parent template.
  kube_master_ip:
    type: string
    description: IP address of the Kubernetes master server.
  fixed_network_id:
    type: string
    description: Network from which to allocate fixed addresses.
  fixed_subnet_id:
    type: string
    description: Subnet from which to allocate fixed addresses.

resources:

  node_wait_handle:
    type: "AWS::CloudFormation::WaitConditionHandle"

  node_wait_condition:
    type: "AWS::CloudFormation::WaitCondition"
    depends_on:
      - kube_node
    properties:
      Handle:
        get_resource: node_wait_handle
      Timeout: "6000"

  # I am lazy, so this opens up all ports.
  secgroup_all_open:
    type: "OS::Neutron::SecurityGroup"
    properties:
      rules:
        - protocol: icmp
        - protocol: tcp
        - protocol: udp

  kube_node:
    type: "OS::Nova::Server"
    properties:
      image:
        get_param: server_image
      flavor:
        get_param: server_flavor
      key_name:
        get_param: ssh_key_name
      user_data_format: RAW
      user_data:
        str_replace:
          template: |
            #!/bin/sh

            myip=$(ip addr show eth0 |
              awk '$1 == "inet" {print $2}' | cut -f1 -d/)
            myip_last_octet=${myip##*.}

            sed -i '
              /^KUBE_ALLOW_PRIV=/ s/=.*/="--allow_privileged=$ALLOW_PRIV"/
            ' /etc/kubernetes/config

            sed -i '/^KUBE_ETCD_SERVERS=/ s|=.*|="--etcd_servers=http://$KUBE_MASTER_IP:4001"|' \
              /etc/kubernetes/config

            sed -i '
              /^KUBELET_ADDRESS=/ s/=.*/="--address=0.0.0.0"/
              /^KUBELET_HOSTNAME=/ s/=.*/="--hostname_override='"$myip"'"/
            ' /etc/kubernetes/kubelet

            sed -i '
              /^KUBE_MASTER=/ s/=.*/="--master=$KUBE_MASTER_IP:8080"/
            ' /etc/kubernetes/apiserver

            sed -i '
              /^FLANNEL_ETCD=/ s|=.*|="http://$KUBE_MASTER_IP:4001"|
            ' /etc/sysconfig/flanneld

            cat >> /etc/environment <<EOF
            KUBERNETES_MASTER=http://$KUBE_MASTER_IP:8080
            EOF

            # make centos user a member of the docker group
            # (so you can run docker commands as the centos user)
            if ! grep -q docker /etc/group; then
              grep docker /lib/group >> /etc/group
            fi
            usermod -G docker centos

            cat > /usr/local/bin/flanneld-waiter <<'EOF'
            #!/bin/sh
            
            while ! [ -f /run/flannel/subnet.env ]; do
              echo "waiting for flanneld"
              sleep 1
            done

            echo flanneld is active

            exit 0
            EOF

            chmod 755 /usr/local/bin/flanneld-waiter

            cat > /etc/systemd/system/flanneld-waiter.service <<'EOF'
            [Unit]
            Description=Wait for flanneld to provide subnet/mtu information
            After=network.target flanneld.service
            Requires=flanneld.service

            [Service]
            Type=oneshot
            ExecStart=/usr/local/bin/flanneld-waiter

            [Install]
            WantedBy=multi-user.target
            EOF

            cat > /etc/systemd/system/docker.service <<'EOF'
            [Unit]
            Description=Docker Application Container Engine
            Documentation=http://docs.docker.com
            After=network.target docker.socket flanneld-waiter.service
            Requires=docker.socket flanneld-waiter.service

            [Service]
            Type=notify
            EnvironmentFile=-/etc/sysconfig/docker
            EnvironmentFile=-/etc/sysconfig/docker-storage
            EnvironmentFile=-/run/flannel/subnet.env
            ExecStart=/usr/bin/docker -d -H fd:// --bip $FLANNEL_SUBNET --mtu $FLANNEL_MTU $OPTIONS $DOCKER_STORAGE_OPTIONS
            Restart=on-failure
            LimitNOFILE=1048576
            LimitNPROC=1048576

            [Install]
            WantedBy=multi-user.target
            EOF

            echo reloading systemd
            systemctl daemon-reload

            # docker is already enabled and possibly running on centos atomic host
            # so we need to stop it first and delete the docker0 bridge (which will
            # be re-created using the flannel-provided subnet).
            echo stopping docker
            systemctl stop docker
            ip link del docker0

            echo starting services
            for service in flanneld-waiter flanneld docker.socket kubelet kube-proxy; do
              systemctl enable $service
              systemctl --no-block start $service
            done

            echo notifying heat
            curl -sf -X PUT -H 'Content-Type: application/json' \
              --data-binary '{"Status": "SUCCESS",
                "Reason": "Setup complete",
                "Data": "OK", "UniqueId": "00000"}' \
              "$WAIT_HANDLE"
          params:
            "$ALLOW_PRIV": {get_param: allow_priv}
            "$KUBE_MASTER_IP": {get_param: kube_master_ip}
            "$WAIT_HANDLE": {get_resource: node_wait_handle}
      networks:
        - port:
            get_resource: kube_node_eth0

  kube_node_eth0:
    type: "OS::Neutron::Port"
    properties:
      network_id:
        get_param: fixed_network_id
      security_groups:
        - get_resource: secgroup_all_open
      fixed_ips:
        - subnet_id:
            get_param: fixed_subnet_id

  kube_node_floating:
    type: "OS::Neutron::FloatingIP"
    properties:
      floating_network_id:
        get_param: external_network_id
      port_id:
        get_resource: kube_node_eth0

outputs:

  kube_node_ip:
    value: {get_attr: [kube_node_eth0, fixed_ips, 0, ip_address]}
  
  kube_node_external_ip:
    value: {get_attr: [kube_node_floating, floating_ip_address]}

